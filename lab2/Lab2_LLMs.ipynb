{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
      "metadata": {
        "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525"
      },
      "source": [
        "# Introduction\n",
        "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
        "\n",
        "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
        "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
        "\n",
        "# Exercise 1: Warming Up\n",
        "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 GPT"
      ],
      "metadata": {
        "id": "nHkzyD74EugR"
      },
      "id": "nHkzyD74EugR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.1 Importing libraries"
      ],
      "metadata": {
        "id": "AyDdPMTWFLBB"
      },
      "id": "AyDdPMTWFLBB"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n"
      ],
      "metadata": {
        "id": "oBpA1xZEFqPe"
      },
      "id": "oBpA1xZEFqPe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.2 Hyperparameters"
      ],
      "metadata": {
        "id": "uqfZsewsFR0R"
      },
      "id": "uqfZsewsFR0R"
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 3000 # I noticed a degradatino after 3000 iterations\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------"
      ],
      "metadata": {
        "id": "ThqEHkrIFscX"
      },
      "id": "ThqEHkrIFscX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.3 Data preparation"
      ],
      "metadata": {
        "id": "LWxqfc-RFXns"
      },
      "id": "LWxqfc-RFXns"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "#with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    #text = f.read()\n",
        "\n",
        "with open('divina_commedia.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "SkyUhfWIGgu_"
      },
      "id": "SkyUhfWIGgu_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.4 Evaluation boilerplate"
      ],
      "metadata": {
        "id": "cxHxE6RTFhq7"
      },
      "id": "cxHxE6RTFhq7"
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "Ol1LWYGrGjFe"
      },
      "id": "Ol1LWYGrGjFe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.5 Model definition"
      ],
      "metadata": {
        "id": "DeztEMUxFlBe"
      },
      "id": "DeztEMUxFlBe"
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple GPT\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # Weights initialization dependeding of the type of layer and the activation function\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            if module.weight is not None:\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "        elif isinstance(module, nn.BatchNorm2d):\n",
        "            if module.weight is not None:\n",
        "                nn.init.constant_(module.weight, 1)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "tmPZEomdGknh"
      },
      "id": "tmPZEomdGknh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1.6 Training"
      ],
      "metadata": {
        "id": "Ai4jICrFFnwk"
      },
      "id": "Ai4jICrFFnwk"
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "print('Training is over: text generation from the model...')\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "hoelkOrFY8bN"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hoelkOrFY8bN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perplexity**"
      ],
      "metadata": {
        "id": "HxxZZRXmLOh-"
      },
      "id": "HxxZZRXmLOh-"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "t6cVm_zCNAVo"
      },
      "id": "t6cVm_zCNAVo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your GPT model in evaluation mode\n",
        "m.eval()\n",
        "\n",
        "def calculate_perplexity(model, dataset):\n",
        "    total_loss = 0.0\n",
        "    total_words = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for k in range(len(dataset)):\n",
        "            X, y = get_batch('val')\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass to get logits\n",
        "            logits, loss = model(X, y)\n",
        "\n",
        "            total_loss += loss.item() * y.numel()\n",
        "            total_words += y.numel()\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = torch.exp(total_loss / torch.tensor(total_words))\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "\n",
        "#perplexity_train = calculate_perplexity(m, train_data)\n",
        "perplexity_val = calculate_perplexity(m, val_data)\n",
        "\n",
        "#print(f\"Perplexity on Train Set: {perplexity_train:.2f}\")\n",
        "print(f\"Perplexity on Validation Set: {perplexity_val:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vB_KmNNENCWA",
        "outputId": "2936f9f0-c59f-4d6f-def0-bca3284cdc0f"
      },
      "id": "vB_KmNNENCWA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity on Validation Set: 4.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFuyYjQwZDI2"
      },
      "source": [
        "# Exercise 2: Working with Real LLMs\n",
        "\n",
        "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
        "\n",
        "## Exercise 2.1: Installation and text tokenization\n",
        "\n",
        "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
        "\n",
        "    conda install -c huggingface -c conda-forge transformers\n",
        "    \n",
        "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text).\n",
        "\n",
        "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
        "\n",
        "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
      ],
      "id": "CFuyYjQwZDI2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Install HuggingFace"
      ],
      "metadata": {
        "id": "ideHjduxZDI3"
      },
      "id": "ideHjduxZDI3"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf84dc4-b824-4f1e-e9b2-3cdd31c1f94c",
        "id": "krqMB8kRZDI3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "id": "krqMB8kRZDI3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.2: Select the prompt"
      ],
      "metadata": {
        "id": "bLU8tucFZDI5"
      },
      "id": "bLU8tucFZDI5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for text generation\n",
        "dante_prompt = \"In the middle of the journey of our life, I found myself in a\"\n",
        "theweeknd_prompt = \"I was born in a city Where the winter nights don't ever sleep So this life's always with me The ice inside my veins will never bleed My, ooh My, ooh Uh, every time you try to fix me I know you'll never find that missing piece When you cry and say you miss me I'll lie and tell you that I'll never leave But I sacrificed (sacrificed) Your love for more of the night (of the night) I try to put up a fight (up a fight) Can't tie me down (down) I don't wanna sacrifice For your love, I try I don't wanna sacrifice But I love my time My, ooh My, ooh\"\n",
        "martin_luther_king_prompt = \"I have a dream that one day every valley shall be engulfed, every hill shall be exalted and every mountain shall be made low, the rough places will be made plains and the crooked places will be made straight and the glory of the Lord shall be revealed and all flesh shall see it together\""
      ],
      "metadata": {
        "id": "30E_tvDkZDI5"
      },
      "execution_count": null,
      "outputs": [],
      "id": "30E_tvDkZDI5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3: Text Tokenizer"
      ],
      "metadata": {
        "id": "u0w5Z-2IZDI6"
      },
      "id": "u0w5Z-2IZDI6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Required Modules\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Instantiate the GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "b7184b61b71942f88f2b75c6e43e35b8",
            "9a2787d3603244e48bf35637b99c2172",
            "72e0fc301d2846449d7daff4861d8e22",
            "792325fa94d24bdda5b55c1d57653df1",
            "9bac545b74334fac88df5d1962e66dc6",
            "de51e61baade4adda2f2bdfb5c3c0d40",
            "1cacf4d6601a4983af6e4fdd7db5f010",
            "0b9ab51002654c5d9478bd0a830da9de",
            "97747426580f4713b20dcc6f91f12c6d",
            "f76a982ef921456f97a342c0db4917af",
            "e128f5c35324474f8355a733c946cdee",
            "25aca666d87349bda4ac982c22327bb0",
            "33f0a4db881241d28a9572cf6aa3c325",
            "6cdb2a3504344523a8221bf131ab754e",
            "b3f7fedb18b6416eb775b7a161bbb049",
            "83aa57cd305346b7b80b19abfed30634",
            "2f31bf181bf24f089291d4500a57f9bb",
            "25d6d559e7ca43acb34383512fb5a116",
            "7af853adcbed40fdb2580c621e6112b5",
            "e84f84c3514c4f2a9f30deb4b847c3d5",
            "151dee95fd62482c91213818dcb231d8",
            "bc2ca105699c42ed8c021e1db7f376b3",
            "b120adb57d12445aa82583cdd4987506",
            "155b83f4c30a4dcb92c8cd1c5fe3559c",
            "c3deba2008d64ed8b44f3f5aad00709e",
            "b517419a437442dda72f7956b167b11f",
            "b608c423e58f4d1497ef5a327a0bbef6",
            "747a7168411747069c61aee6768fc9c1",
            "06aba3b0bd324e5eaa10e580d0a8fb1b",
            "17b20383c87e4c9793be7b5b616d5270",
            "72a878c10188452994038e480d605690",
            "aeebb591db7b4bfaa7b14fc06b813ff1",
            "5e13cba922954ae3af4dd0ac141ca5b1"
          ]
        },
        "id": "a6cg-3eyRd8O",
        "outputId": "7f30b38d-8e1b-41b3-cb03-347e4324d355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7184b61b71942f88f2b75c6e43e35b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25aca666d87349bda4ac982c22327bb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b120adb57d12445aa82583cdd4987506"
            }
          },
          "metadata": {}
        }
      ],
      "id": "a6cg-3eyRd8O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dante Tokenizer**"
      ],
      "metadata": {
        "id": "MyCZYpD7RgSf"
      },
      "id": "MyCZYpD7RgSf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391694af-23c8-40a2-9362-ccda4ce6f378",
        "id": "SIDGQyhPZDI8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:  In the middle of the journey of our life, I found myself in a\n",
            "Encoded Tokens:  tensor([[ 818,  262, 3504,  286,  262, 7002,  286,  674, 1204,   11,  314, 1043,\n",
            "         3589,  287,  257]])\n",
            "Input Text Length:  61\n",
            "Encoded Sequence Length:  15\n"
          ]
        }
      ],
      "source": [
        "# Encode Text into Integer Tokens\n",
        "input_text = dante_prompt\n",
        "\n",
        "# Encode the text\n",
        "dante_encoded_text = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "# Print the input text\n",
        "print(\"Input Text: \", input_text)\n",
        "\n",
        "# Print the encoded tokens\n",
        "print(\"Encoded Tokens: \", dante_encoded_text['input_ids'])\n",
        "\n",
        "# Compare the length of input with the encoded sequence length\n",
        "print(\"Input Text Length: \", len(input_text))\n",
        "print(\"Encoded Sequence Length: \", dante_encoded_text['input_ids'].shape[1])\n"
      ],
      "id": "SIDGQyhPZDI8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Weeknd Tokenizer**"
      ],
      "metadata": {
        "id": "YSWqTIpuRlzx"
      },
      "id": "YSWqTIpuRlzx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Text into Integer Tokens\n",
        "input_text = theweeknd_prompt\n",
        "\n",
        "# Encode the text\n",
        "tw_encoded_text = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "# Print the input text\n",
        "print(\"Input Text: \", input_text)\n",
        "\n",
        "# Print the encoded tokens\n",
        "print(\"Encoded Tokens: \", tw_encoded_text['input_ids'])\n",
        "\n",
        "# Compare the length of input with the encoded sequence length\n",
        "print(\"Input Text Length: \", len(input_text))\n",
        "print(\"Encoded Sequence Length: \", tw_encoded_text['input_ids'].shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVwtCwdjRosh",
        "outputId": "04e67ef8-3e5a-406f-cbc1-8853457af5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:  I was born in a city Where the winter nights don't ever sleep So this life's always with me The ice inside my veins will never bleed My, ooh My, ooh Uh, every time you try to fix me I know you'll never find that missing piece When you cry and say you miss me I'll lie and tell you that I'll never leave But I sacrificed (sacrificed) Your love for more of the night (of the night) I try to put up a fight (up a fight) Can't tie me down (down) I don't wanna sacrifice For your love, I try I don't wanna sacrifice But I love my time My, ooh My, ooh\n",
            "Encoded Tokens:  tensor([[   40,   373,  4642,   287,   257,  1748,  6350,   262,  7374, 12513,\n",
            "           836,   470,  1683,  3993,  1406,   428,  1204,   338,  1464,   351,\n",
            "           502,   383,  4771,  2641,   616, 32375,   481,  1239, 30182,  2011,\n",
            "            11,   267,  1219,  2011,    11,   267,  1219, 28574,    11,   790,\n",
            "           640,   345,  1949,   284,  4259,   502,   314,   760,   345,  1183,\n",
            "          1239,  1064,   326,  4814,  3704,  1649,   345,  3960,   290,   910,\n",
            "           345,  2051,   502,   314,  1183,  6486,   290,  1560,   345,   326,\n",
            "           314,  1183,  1239,  2666,   887,   314, 27445,   357, 30584,    81,\n",
            "           811,   276,     8,  3406,  1842,   329,   517,   286,   262,  1755,\n",
            "           357,  1659,   262,  1755,     8,   314,  1949,   284,  1234,   510,\n",
            "           257,  1907,   357,   929,   257,  1907,     8,  1680,   470,  9839,\n",
            "           502,   866,   357,  2902,     8,   314,   836,   470, 18869, 11728,\n",
            "          1114,   534,  1842,    11,   314,  1949,   314,   836,   470, 18869,\n",
            "         11728,   887,   314,  1842,   616,   640,  2011,    11,   267,  1219,\n",
            "          2011,    11,   267,  1219]])\n",
            "Input Text Length:  545\n",
            "Encoded Sequence Length:  144\n"
          ]
        }
      ],
      "id": "zVwtCwdjRosh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Martin Luther King Tokenizer**"
      ],
      "metadata": {
        "id": "QfMn_GOVSCv_"
      },
      "id": "QfMn_GOVSCv_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Text into Integer Tokens\n",
        "input_text = martin_luther_king_prompt\n",
        "\n",
        "# Encode the text\n",
        "mlk_encoded_text = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "# Print the input text\n",
        "print(\"Input Text: \", input_text)\n",
        "\n",
        "# Print the encoded tokens\n",
        "print(\"Encoded Tokens: \", mlk_encoded_text['input_ids'])\n",
        "\n",
        "# Compare the length of input with the encoded sequence length\n",
        "print(\"Input Text Length: \", len(input_text))\n",
        "print(\"Encoded Sequence Length: \", mlk_encoded_text['input_ids'].shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrV0TmIZSHOc",
        "outputId": "98b3f529-7eb5-4049-c049-57f207b471b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:  I have a dream that one day every valley shall be engulfed, every hill shall be exalted and every mountain shall be made low, the rough places will be made plains and the crooked places will be made straight and the glory of the Lord shall be revealed and all flesh shall see it together\n",
            "Encoded Tokens:  tensor([[   40,   423,   257,  4320,   326,   530,  1110,   790, 19272,  2236,\n",
            "           307, 40997,    11,   790, 12788,  2236,   307, 46683,   290,   790,\n",
            "          8598,  2236,   307,   925,  1877,    11,   262,  5210,  4113,   481,\n",
            "           307,   925, 36149,   290,   262, 45571,  4113,   481,   307,   925,\n",
            "          3892,   290,   262, 13476,   286,   262,  4453,  2236,   307,  4602,\n",
            "           290,   477, 11222,  2236,   766,   340,  1978]])\n",
            "Input Text Length:  287\n",
            "Encoded Sequence Length:  57\n"
          ]
        }
      ],
      "id": "TrV0TmIZSHOc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8mWIV8xZDJB"
      },
      "source": [
        "## Exercise 2.2: Generating Text\n",
        "\n",
        "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
        "\n",
        "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
      ],
      "id": "P8mWIV8xZDJB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2: Instantiate the pre-trained GPT2 model"
      ],
      "metadata": {
        "id": "7JvbBgImZDJC"
      },
      "id": "7JvbBgImZDJC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import GPT2LMHeadModel\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Instantiate the model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "89a410e613694d5598cc481b95b30584",
            "5c0fde50c71545d581a3df2ec05b3d5e",
            "c4aab914c06a47d6830dba2586383d89",
            "6092bea196c3485e87e43e37f973e3d3",
            "8af316f2c5a4476292c8ccbba1b85cb3",
            "f9a81bec01f84ad7ba0627ebf75c33ef",
            "a8ca662d74ef4a339fbc916e740312ea",
            "196cc647e4df4ecfae3d5eee28a1a7c2",
            "22c22311306f4f1c9b77d842761a9464",
            "182bac4fa238465182ca456d6a705122",
            "c375d9102d29485691cfc77e1c02ca87",
            "42809d0cb9b0422bb78934d59918ba9e",
            "68a9fc6407cd41f58328d2abe8c3700e",
            "3d3cded8ecb442b1b7f1f5f4e8ae8d12",
            "2426baa714d7421bb34540b33507c19f",
            "32f1c3495e004d8ba8a3d93ca6a4c94d",
            "b13dd54ffb6f478690d7272118eadde7",
            "a62233d1938b415d886d76c127b9dc24",
            "e9cd346c2d824893a73b32aeb6154a24",
            "7b3d5db8809942b1a96189cf6f930338",
            "a6d03b7542f74bc98fe5b34a7f7838b7",
            "2e19c9bf7f004122a40162a24c5f4ce4"
          ]
        },
        "id": "urIwDr02TEZQ",
        "outputId": "4f5b00ea-78fe-43f2-dbeb-6abd471e9f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89a410e613694d5598cc481b95b30584"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42809d0cb9b0422bb78934d59918ba9e"
            }
          },
          "metadata": {}
        }
      ],
      "id": "urIwDr02TEZQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dante Text Generator**"
      ],
      "metadata": {
        "id": "_SRmeeXJTHbK"
      },
      "id": "_SRmeeXJTHbK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for text generation\n",
        "dante_prompt = \"In the middle of the journey of our life, I found myself in a\""
      ],
      "metadata": {
        "id": "pDRP83AnU0GE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pDRP83AnU0GE"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "test_prompt = dante_prompt\n",
        "test_ids = tokenizer.encode(test_prompt, return_tensors='pt')\n",
        "\n",
        "\n",
        "# Generate text using the GPT2 model\n",
        "output = model.generate(test_ids, max_length=150, num_return_sequences=1, do_sample=True, temperature=0.3)\n",
        "\n",
        "# Decode the generated text tokens and convert them to a string\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\\n\", generated_text)"
      ],
      "metadata": {
        "id": "cQXify-FUbY8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cQXify-FUbY8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Weeknd Text Generator**"
      ],
      "metadata": {
        "id": "FGfCzRi6TLIg"
      },
      "id": "FGfCzRi6TLIg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for text generation\n",
        "theweeknd_prompt = \"I was born in a city Where the winter nights don't ever sleep So this life's always with me The ice inside my veins will never bleed My, ooh My, ooh Uh, every time you try to fix me I know you'll never find that missing piece When you cry and say you miss me I'll lie and tell you that I'll never leave But I sacrificed (sacrificed) Your love for more of the night (of the night) I try to put up a fight (up a fight) Can't tie me down (down) I don't wanna sacrifice For your love, I try I don't wanna sacrifice But I love my time My, ooh My, ooh\"\n"
      ],
      "metadata": {
        "id": "Hda1oqu4VGHm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Hda1oqu4VGHm"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "test_prompt = theweeknd_prompt\n",
        "test_ids = tokenizer.encode(test_prompt, return_tensors='pt')\n",
        "\n",
        "\n",
        "# Generate text using the GPT2 model\n",
        "output = model.generate(test_ids, max_length=150, num_return_sequences=1, do_sample=True, temperature=0.9)\n",
        "\n",
        "# Decode the generated text tokens and convert them to a string\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\\n\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8372cb-7742-4021-f758-278dfcf6922d",
        "id": "nay4d0CUXQjH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " I was born in a city Where the winter nights don't ever sleep So this life's always with me The ice inside my veins will never bleed My, ooh My, ooh Uh, every time you try to fix me I know you'll never find that missing piece When you cry and say you miss me I'll lie and tell you that I'll never leave But I sacrificed (sacrificed) Your love for more of the night (of the night) I try to put up a fight (up a fight) Can't tie me down (down) I don't wanna sacrifice For your love, I try I don't wanna sacrifice But I love my time My, ooh My, ooh Uh, every time you try\n"
          ]
        }
      ],
      "id": "nay4d0CUXQjH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Martin Luther King Text Generator**"
      ],
      "metadata": {
        "id": "JTOujqdoTO_2"
      },
      "id": "JTOujqdoTO_2"
    },
    {
      "cell_type": "code",
      "source": [
        "martin_luther_king_prompt = \"I have a dream that one day every valley shall be engulfed, every hill shall be exalted and every mountain shall be made low, the rough places will be made plains and the crooked places will be made straight and the glory of the Lord shall be revealed and all flesh shall see it together\""
      ],
      "metadata": {
        "id": "qrHk1QTJVNrd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qrHk1QTJVNrd"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt\n",
        "test_prompt = martin_luther_king_prompt\n",
        "test_ids = tokenizer.encode(test_prompt, return_tensors='pt')\n",
        "\n",
        "\n",
        "# Generate text using the GPT2 model\n",
        "output = model.generate(test_ids, max_length=150, num_return_sequences=1, do_sample=True, temperature=0.9)\n",
        "\n",
        "# Decode the generated text tokens and convert them to a string\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\\n\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81828676-8bab-41aa-af7f-6d82f1a54110",
        "id": "uTP6xEB7XVXM"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " I have a dream that one day every valley shall be engulfed, every hill shall be exalted and every mountain shall be made low, the rough places will be made plains and the crooked places will be made straight and the glory of the Lord shall be revealed and all flesh shall see it together with the glory of God; for I know ye not that ye have heard these words of mine Father in Heaven; and the Lord your God shall say unto you, Go up and be a man, and make him a man; and he that hath it in his heart shall he cast out his hand, and he that hath it out thereof shall set it ablaze; and if anyone that hath it in his heart cast out the hand of the Lord shall cast it\n"
          ]
        }
      ],
      "id": "uTP6xEB7XVXM"
    },
    {
      "cell_type": "markdown",
      "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
      "metadata": {
        "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4"
      },
      "source": [
        "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
        "\n",
        "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
        "\n",
        "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
        "\n",
        "+ BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
        "\n",
        "+ The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
        "\n",
        "# Exercise 3.1: Training a Text Classifier (easy)\n",
        "\n",
        "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
        "\n",
        "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model.\n",
        "\n",
        "# Exercise 3.2: Training a Question Answering Model (harder)\n",
        "\n",
        "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch).\n",
        "\n",
        "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
        "\n",
        "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
        "\n",
        "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.0: Training a text classifier"
      ],
      "metadata": {
        "id": "YB4LxO_nj0pR"
      },
      "id": "YB4LxO_nj0pR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `DistillBERT` from HugginFace on the `imdb` dataset to extract features and train a shallow MLP to classify them."
      ],
      "metadata": {
        "id": "rovbv93nsVz7"
      },
      "id": "rovbv93nsVz7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1: Importing libraries"
      ],
      "metadata": {
        "id": "eivzm3zxr8Rh"
      },
      "id": "eivzm3zxr8Rh"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "arr9k426ueu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856f9e29-c6b7-4c4e-f12d-43fb11886323"
      },
      "id": "arr9k426ueu0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.3 dill-0.3.7 huggingface-hub-0.16.4 multiprocess-0.70.15 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "sch9syc2x-u1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf1b1e6-d74a-44cf-d682-46779bb17c40"
      },
      "id": "sch9syc2x-u1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/7.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/7.4 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m6.7/7.4 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, transformers\n",
            "Successfully installed safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "O1xlRSI9rcSg"
      },
      "id": "O1xlRSI9rcSg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2: Hyperparameters"
      ],
      "metadata": {
        "id": "lfCpDgoksDGl"
      },
      "id": "lfCpDgoksDGl"
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "dUkXC8jGsISU"
      },
      "id": "dUkXC8jGsISU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3: Instantiate the tokenizer and the model"
      ],
      "metadata": {
        "id": "FHF0CbnQsKX4"
      },
      "id": "FHF0CbnQsKX4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I had to put `max_length' hyperparameter because apparentely the RAM memory was completely full filled until the runtime crashed"
      ],
      "metadata": {
        "id": "gzY5NyyshwXD"
      },
      "id": "gzY5NyyshwXD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the DistilBERT tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', max_length = 512)\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "jNfAwW58rHGK"
      },
      "id": "jNfAwW58rHGK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfB0gO6JMoRV",
        "outputId": "15fe4481-cfd8-4352-ec01-9a0e401984cd"
      },
      "id": "YfB0gO6JMoRV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.4: Data Preparation"
      ],
      "metadata": {
        "id": "SAhjHhI6nmQS"
      },
      "id": "SAhjHhI6nmQS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset\n",
        "dataset = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "2glZZxQ5tuJB"
      },
      "id": "2glZZxQ5tuJB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce the number of samples to avoid GPU memory saturation."
      ],
      "metadata": {
        "id": "5bB0yQB4M1xD"
      },
      "id": "5bB0yQB4M1xD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid to do that I tried several solutions, including using a dataloader, reducing the batch size, using a gradient accumulation in the process of feature extraction. Eventually, I had to reduce the number of samples."
      ],
      "metadata": {
        "id": "JYBt66E2M5gi"
      },
      "id": "JYBt66E2M5gi"
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_train_dataset = dataset[\"train\"].shuffle(seed=42)[:2500]\n",
        "reduced_test_dataset = dataset[\"test\"].shuffle(seed=42)[:500]"
      ],
      "metadata": {
        "id": "1WVev62gxpAK"
      },
      "id": "1WVev62gxpAK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data = reduced_train_dataset\n",
        "test_data = reduced_test_dataset\n",
        "# Split train_data into train and validation sets\n",
        "#train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Step 2: Tokenize the text and convert it into PyTorch tensors\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', max_length=128)\n",
        "\n",
        "\n",
        "# Tokenize the text and convert it to PyTorch tensors\n",
        "def tokenize_text(text):\n",
        "    encoded = tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
        "    input_ids = encoded['input_ids']\n",
        "    attention_mask = encoded['attention_mask']\n",
        "    return input_ids, attention_mask\n",
        "\n",
        "# Prepare the data and create DataLoaders\n",
        "train_input_ids, train_attention_mask = tokenize_text(train_data['text'])\n",
        "train_labels = torch.tensor(train_data['label'], dtype=torch.long)\n",
        "\n",
        "test_input_ids, test_attention_mask = tokenize_text(test_data['text'])\n",
        "test_labels = torch.tensor(test_data['label'], dtype=torch.long)\n",
        "\n",
        "#val_input_ids, val_attention_mask = tokenize_text(val_data['text'])\n",
        "#val_labels = torch.tensor(val_data['label'], dtype=torch.long)\n",
        "\n"
      ],
      "metadata": {
        "id": "vfQK38QDnsGK"
      },
      "id": "vfQK38QDnsGK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "train_dataloader = DataLoader(TensorDataset(train_input_ids, train_attention_mask, train_labels), batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(TensorDataset(test_input_ids, test_attention_mask, test_labels), batch_size=batch_size)\n",
        "#val_dataloader = DataLoader(TensorDataset(val_input_ids, val_attention_mask, val_labels), batch_size=batch_size)"
      ],
      "metadata": {
        "id": "PBW3PT3Ks9R6"
      },
      "id": "PBW3PT3Ks9R6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.5: Feature extractor"
      ],
      "metadata": {
        "id": "JoYbDVvkq1tc"
      },
      "id": "JoYbDVvkq1tc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First version of feature extractor that is used, without gradient accumulator. Since it was not working (i.e. the GPU memory got saturated very fast) I had to change it and use a different one. I leave it here just for reference."
      ],
      "metadata": {
        "id": "Y5hj8O_21FHS"
      },
      "id": "Y5hj8O_21FHS"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract features (representations) from the model\n",
        "def extract_features(dataloader):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in dataloader:\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            features_list.append(outputs.last_hidden_state)\n",
        "            labels_list.append(labels)\n",
        "\n",
        "    features = torch.cat(features_list, dim=0)\n",
        "    labels = torch.cat(labels_list, dim=0)\n",
        "\n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "IPlSH6GV1Gkq"
      },
      "id": "IPlSH6GV1Gkq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second version with gradient accumulator."
      ],
      "metadata": {
        "id": "DPSzuhEh1Hqn"
      },
      "id": "DPSzuhEh1Hqn"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(dataloader, gradient_accumulation_steps=2):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (input_ids, attention_mask, labels) in enumerate(dataloader):\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            features_list.append(outputs.last_hidden_state)\n",
        "            labels_list.append(labels)\n",
        "\n",
        "            # Perform gradient accumulation every gradient_accumulation_steps batches\n",
        "            if (i + 1) % gradient_accumulation_steps == 0:\n",
        "                features = torch.cat(features_list, dim=0)\n",
        "                labels = torch.cat(labels_list, dim=0)\n",
        "\n",
        "                yield features, labels\n",
        "\n",
        "                features_list = []\n",
        "                labels_list = []\n",
        "\n",
        "        # Perform the final gradient accumulation for any remaining batches\n",
        "        if len(features_list) > 0:\n",
        "            features = torch.cat(features_list, dim=0)\n",
        "            labels = torch.cat(labels_list, dim=0)\n",
        "\n",
        "            yield features, labels\n",
        "\n",
        "    # At the end of the function, return the collected features and labels as tensors\n",
        "    features = torch.cat(features_list, dim=0)\n",
        "    labels = torch.cat(labels_list, dim=0)\n",
        "    return features, labels\n"
      ],
      "metadata": {
        "id": "cWgj3KGA02H2"
      },
      "id": "cWgj3KGA02H2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, train_labels = list(extract_features(train_dataloader))[0]\n",
        "test_features, test_labels = list(extract_features(test_dataloader))[0]\n",
        "\n",
        "#train_features, train_labels = extract_features(train_dataloader)\n",
        "#test_features, test_labels = extract_features(test_dataloader)\n",
        "#val_features, val_labels = extract_features(val_dataloader)\n",
        "\n",
        "# Now you have the extracted features for training and validation sets\n",
        "print(\"Train Features:\", train_features.size())\n",
        "print(\"Train Labels:\", train_labels.size())\n",
        "print(\"Test Features:\", test_features.size())\n",
        "print(\"Test Labels:\", test_labels.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJDkp-Sgq5w-",
        "outputId": "33038a81-97f8-4148-aecf-fd7f81cf7ebc"
      },
      "id": "rJDkp-Sgq5w-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Features: torch.Size([16, 512, 768])\n",
            "Train Labels: torch.Size([16])\n",
            "Test Features: torch.Size([16, 512, 768])\n",
            "Test Labels: torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.6: Text Classifiers\n",
        "In this section we present the text classifiers training. We will train different model classifiers and compare their performance. Since our goal is not to focus on the architecture of the classifiers, I decided to use some pre built models from scikit learn.\n",
        "\n",
        "\n",
        "\n",
        "1.   MLPClassifier\n",
        "2.   Logistic Regression\n",
        "3.   SVM\n",
        "4.   Random Forest\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IJ2HwydJ3fQN"
      },
      "id": "IJ2HwydJ3fQN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since to train our classifier we are using models from the scikit learn library, they are not compatible with torch tensor. We have to convert them to numpy array"
      ],
      "metadata": {
        "id": "fxAc60gK5SK4"
      },
      "id": "fxAc60gK5SK4"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Convert PyTorch tensors to NumPy arrays and flatten them\n",
        "train_features_np = train_features.cpu().numpy().reshape(len(train_features), -1)\n",
        "train_labels_np = train_labels.cpu().numpy()\n",
        "test_features_np = test_features.cpu().numpy().reshape(len(test_features), -1)\n",
        "test_labels_np = test_labels.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "w2be_Qt84-je"
      },
      "id": "w2be_Qt84-je",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP**"
      ],
      "metadata": {
        "id": "yDhuCIHS40pI"
      },
      "id": "yDhuCIHS40pI"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "# Instantiate the MLP Classifier\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=100, random_state=42)\n",
        "\n",
        "# Train the classifier on the extracted features\n",
        "mlp_classifier.fit(train_features_np, train_labels_np)\n",
        "\n",
        "# Evaluate the classifier on the test set\n",
        "mlp_accuracy = mlp_classifier.score(test_features_np, test_labels_np)\n",
        "print(\"MLP Accuracy:\", mlp_accuracy)\n"
      ],
      "metadata": {
        "id": "4eE_L7FA3izC"
      },
      "id": "4eE_L7FA3izC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "MSDZBXHr44fH"
      },
      "id": "MSDZBXHr44fH"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# Instantiate the Logistic Regression Classifier\n",
        "logreg_classifier = LogisticRegression(max_iter=50, random_state=42)\n",
        "\n",
        "# Train the classifier on the extracted features\n",
        "logreg_classifier.fit(train_features_np, train_labels_np)\n",
        "\n",
        "# Evaluate the classifier on the test set\n",
        "logreg_accuracy = logreg_classifier.score(test_features_np, test_labels_np)\n",
        "print(\"Logistic Regression Accuracy:\", logreg_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "8ms1d3135HZu"
      },
      "id": "8ms1d3135HZu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is useful to check overfitting."
      ],
      "metadata": {
        "id": "cPj7H6CpODut"
      },
      "id": "cPj7H6CpODut"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Create a learning curve with 5 different training set sizes (e.g., 20%, 40%, 60%, 80%, 100%)\n",
        "train_sizes, train_scores, valid_scores = learning_curve(\n",
        "    logreg_classifier, train_features_np, train_labels_np, cv=5, train_sizes=np.linspace(0.2, 1.0, 5)\n",
        ")\n",
        "\n",
        "# Compute mean and standard deviation of training and validation scores across the 5 folds\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "valid_scores_mean = np.mean(valid_scores, axis=1)\n",
        "valid_scores_std = np.std(valid_scores, axis=1)\n",
        "\n",
        "# Plot the learning curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"Logistic Regression Learning Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qgzouvUH79Em"
      },
      "id": "qgzouvUH79Em",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "PZd2S-kS43Ct"
      },
      "id": "PZd2S-kS43Ct"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Instantiate the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the classifier on the extracted features\n",
        "svm_classifier.fit(train_features_np, train_labels_np)\n",
        "\n",
        "# Evaluate the classifier on the test set\n",
        "svm_accuracy = svm_classifier.score(test_features_np, test_labels_np)\n",
        "print(\"SVM Accuracy:\", svm_accuracy)\n"
      ],
      "metadata": {
        "id": "9r1aNZJu5LP3"
      },
      "id": "9r1aNZJu5LP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "\n",
        "\n",
        "# Create a learning curve with 5 different training set sizes (e.g., 20%, 40%, 60%, 80%, 100%)\n",
        "train_sizes, train_scores, valid_scores = learning_curve(\n",
        "    svm_classifier, train_features_np, train_labels_np, cv=5, train_sizes=np.linspace(0.2, 1.0, 5)\n",
        ")\n",
        "\n",
        "# Compute mean and standard deviation of training and validation scores across the 5 folds\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "valid_scores_mean = np.mean(valid_scores, axis=1)\n",
        "valid_scores_std = np.std(valid_scores, axis=1)\n",
        "\n",
        "# Plot the learning curve\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color=\"g\")\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"SVM Learning Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p7L1I5oq8hCY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "p7L1I5oq8hCY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "VM5sVsnsOK45"
      },
      "id": "VM5sVsnsOK45"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier on the extracted features\n",
        "rf_classifier.fit(train_features_np, train_labels_np)\n",
        "\n",
        "# Evaluate the classifier on the test set\n",
        "rf_accuracy = rf_classifier.score(test_features_np, test_labels_np)\n",
        "print(\"Random Forest Accuracy:\", rf_accuracy)\n"
      ],
      "metadata": {
        "id": "mM4eq4698y9y"
      },
      "id": "mM4eq4698y9y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "iGnlaEdV6VMZ"
      },
      "id": "iGnlaEdV6VMZ"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Predict labels for the test set\n",
        "mlp_predicted_labels = mlp_classifier.predict(test_features_np)\n",
        "logreg_predicted_labels = logreg_classifier.predict(test_features_np)\n",
        "svm_predicted_labels = svm_classifier.predict(test_features_np)\n",
        "rf_predicted_labels = rf_classifier.predict(test_features_np)\n",
        "\n",
        "# Compute confusion matrix\n",
        "mlp_cm = confusion_matrix(test_labels_np, mlp_predicted_labels)\n",
        "logreg_cm = confusion_matrix(test_labels_np, logreg_predicted_labels)\n",
        "svm_cm = confusion_matrix(test_labels_np, svm_predicted_labels)\n",
        "rf_cm = confusion_matrix(test_labels_np, rf_predicted_labels)\n",
        "\n",
        "# Compute classification report\n",
        "mlp_report = classification_report(test_labels_np, mlp_predicted_labels)\n",
        "logreg_report = classification_report(test_labels_np, logreg_predicted_labels)\n",
        "svm_report = classification_report(test_labels_np, svm_predicted_labels)\n",
        "rf_report = classification_report(test_labels_np, rf_predicted_labels)\n",
        "print(\"MLP Confusion Matrix:\")\n",
        "print(mlp_cm)\n",
        "print(\"MLP Classification Report:\")\n",
        "print(mlp_report)\n",
        "\n",
        "print(\"Logistic Regression Confusion Matrix:\")\n",
        "print(logreg_cm)\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(logreg_report)\n",
        "\n",
        "print(\"SVM Confusion Matrix:\")\n",
        "print(svm_cm)\n",
        "print(\"SVM Classification Report:\")\n",
        "print(svm_report)\n",
        "\n",
        "print(\"Random Forest Confusion Matrix:\")\n",
        "print(rf_cm)\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(rf_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma1NCayC63HE",
        "outputId": "d211ff96-8557-4cd4-eae6-756e84353cae"
      },
      "id": "ma1NCayC63HE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Confusion Matrix:\n",
            "[[3 4]\n",
            " [1 8]]\n",
            "MLP Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.43      0.55         7\n",
            "           1       0.67      0.89      0.76         9\n",
            "\n",
            "    accuracy                           0.69        16\n",
            "   macro avg       0.71      0.66      0.65        16\n",
            "weighted avg       0.70      0.69      0.67        16\n",
            "\n",
            "Logistic Regression Confusion Matrix:\n",
            "[[7 0]\n",
            " [8 1]]\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      1.00      0.64         7\n",
            "           1       1.00      0.11      0.20         9\n",
            "\n",
            "    accuracy                           0.50        16\n",
            "   macro avg       0.73      0.56      0.42        16\n",
            "weighted avg       0.77      0.50      0.39        16\n",
            "\n",
            "SVM Confusion Matrix:\n",
            "[[6 1]\n",
            " [8 1]]\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.86      0.57         7\n",
            "           1       0.50      0.11      0.18         9\n",
            "\n",
            "    accuracy                           0.44        16\n",
            "   macro avg       0.46      0.48      0.38        16\n",
            "weighted avg       0.47      0.44      0.35        16\n",
            "\n",
            "Random Forest Confusion Matrix:\n",
            "[[7 0]\n",
            " [9 0]]\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      1.00      0.61         7\n",
            "           1       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.44        16\n",
            "   macro avg       0.22      0.50      0.30        16\n",
            "weighted avg       0.19      0.44      0.27        16\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "CFuyYjQwZDI2",
        "lfCpDgoksDGl",
        "SAhjHhI6nmQS"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}